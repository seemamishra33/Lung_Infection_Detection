{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "\n",
    "from glob import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Initialize Glob to interact with storage\n",
    "\n",
    "image_files = glob(train_path +'/*/*.jp*g')\n",
    "valid_image_files = glob(validation_path +'/*/*.jp*g')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Set the training path and validation path\n",
    "\n",
    "train_path = '../data/raw/train/'\n",
    "validation_path = '../data/raw/test/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Preprocessing --- Goal to make the data compatible for CNN input\n",
    "# In tensorflow, you can achieve the same using ImageGenerators\n",
    "\n",
    "\n",
    "train_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale= 1.0/255.,\n",
    "                                                                  rotation_range=90,\n",
    "                                                                  zoom_range=0.2,\n",
    "                                                                  horizontal_flip=True,\n",
    "                                                                  vertical_flip=True)\n",
    "\n",
    "test_generator = tf.keras.preprocessing.image.ImageDataGenerator(rescale= 1.0/255.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 251 images belonging to 3 classes.\n",
      "Found 66 images belonging to 3 classes.\n"
     ]
    }
   ],
   "source": [
    "#Pass the images through the generator\n",
    "\n",
    "trainImageData = train_generator.flow_from_directory(train_path,\n",
    "                                                     batch_size=32, #how many images to give per iteration in an epoch\n",
    "                                                     class_mode=\"categorical\", #Incase of multi-class classification, \"categorical\"\n",
    "                                                     target_size=(64,64) #Ensures all images are of same size (resizing)\n",
    "                                                     ) \n",
    "\n",
    "\n",
    "testImageData = test_generator.flow_from_directory(validation_path,\n",
    "                                                     batch_size=32, #how many images to give per iteration in an epoch\n",
    "                                                     class_mode=\"categorical\", #Incase of multi-class classification, \"categorical\"\n",
    "                                                     target_size=(64,64) #Ensures all images are of same size (resizing)\n",
    "                                                     ) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(64, 64, 3)\n",
      "(64, 64, 3)\n"
     ]
    }
   ],
   "source": [
    "print(trainImageData.image_shape)\n",
    "print(testImageData.image_shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#trainImageData.filenames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 13:08:00.703900: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:305] Could not identify NUMA node of platform GPU ID 0, defaulting to 0. Your kernel may not have been built with NUMA support.\n",
      "2022-07-31 13:08:00.704911: I tensorflow/core/common_runtime/pluggable_device/pluggable_device_factory.cc:271] Created TensorFlow device (/job:localhost/replica:0/task:0/device:GPU:0 with 0 MB memory) -> physical PluggableDevice (device: 0, name: METAL, pci bus id: <undefined>)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Metal device set to: Apple M1\n"
     ]
    }
   ],
   "source": [
    "# ARchitect the model\n",
    "\n",
    "model = tf.keras.models.Sequential()\n",
    "\n",
    "#Convolution Layer\n",
    "#================================================================================================================================================\n",
    "#First Convolution Layer\n",
    "\n",
    "#Conv2D(noFeatureMap, kernelShape, inputShape, activation, padding) Here same means add one pad layer\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(32 , (3,3) , input_shape= trainImageData.image_shape , activation= 'relu' , padding='same' )) #Convolve\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2))) #Pooling \n",
    "\n",
    "#Second Convolution Layer\n",
    "\n",
    "#Conv2D(noFeatureMap, kernelShape, inputShape, activation, padding) Here same means add one pad layer\n",
    "\n",
    "model.add(tf.keras.layers.Conv2D(16 , (3,3), activation= 'relu' , padding='same' )) #Convolve\n",
    "model.add(tf.keras.layers.MaxPooling2D(pool_size=(2,2))) #Pooling \n",
    "\n",
    "#=================================================================================================================================================\n",
    "# Flatten\n",
    "\n",
    "model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "#=================================================================================================================================================\n",
    "# ANN\n",
    "model.add(tf.keras.layers.Dense(units= 4096 , activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(units= 1024, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dropout(0.2))\n",
    "model.add(tf.keras.layers.Dense(units= 256, activation=\"relu\"))\n",
    "model.add(tf.keras.layers.Dense(units= 1 , activation=\"sigmoid\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " conv2d (Conv2D)             (None, 64, 64, 32)        896       \n",
      "                                                                 \n",
      " max_pooling2d (MaxPooling2D  (None, 32, 32, 32)       0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 32, 32, 16)        4624      \n",
      "                                                                 \n",
      " max_pooling2d_1 (MaxPooling  (None, 16, 16, 16)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 4096)              16781312  \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 4096)              0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 1024)              4195328   \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 1024)              0         \n",
      "                                                                 \n",
      " dense_2 (Dense)             (None, 256)               262400    \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 257       \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 21,244,817\n",
      "Trainable params: 21,244,817\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Compile\n",
    "\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"categorical_crossentropy\",\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "7/7 [==============================] - ETA: 0s - loss: 1.5535 - accuracy: 0.6469"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2022-07-31 13:19:12.933592: I tensorflow/core/grappler/optimizers/custom_graph_optimizer_registry.cc:113] Plugin optimizer for device_type GPU is enabled.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7/7 [==============================] - 7s 702ms/step - loss: 1.5535 - accuracy: 0.6469 - val_loss: 1.2988 - val_accuracy: 0.6667\n",
      "Epoch 2/10\n",
      "7/7 [==============================] - 5s 739ms/step - loss: 1.0719 - accuracy: 0.6743 - val_loss: 1.1035 - val_accuracy: 0.6667\n",
      "Epoch 3/10\n",
      "7/7 [==============================] - 5s 753ms/step - loss: 1.1089 - accuracy: 0.6667 - val_loss: 1.1192 - val_accuracy: 0.6667\n",
      "Epoch 4/10\n",
      "7/7 [==============================] - 5s 737ms/step - loss: 1.0423 - accuracy: 0.6667 - val_loss: 1.1415 - val_accuracy: 0.6198\n",
      "Epoch 5/10\n",
      "7/7 [==============================] - 5s 740ms/step - loss: 1.1745 - accuracy: 0.6423 - val_loss: 1.1304 - val_accuracy: 0.3333\n",
      "Epoch 6/10\n",
      "7/7 [==============================] - 5s 710ms/step - loss: 1.0815 - accuracy: 0.5753 - val_loss: 1.1047 - val_accuracy: 0.6302\n",
      "Epoch 7/10\n",
      "7/7 [==============================] - 5s 745ms/step - loss: 1.1058 - accuracy: 0.5845 - val_loss: 1.1039 - val_accuracy: 0.5885\n",
      "Epoch 8/10\n",
      "7/7 [==============================] - 5s 726ms/step - loss: 0.9783 - accuracy: 0.6804 - val_loss: 1.5795 - val_accuracy: 0.5729\n",
      "Epoch 9/10\n",
      "7/7 [==============================] - 5s 735ms/step - loss: 1.0131 - accuracy: 0.6210 - val_loss: 1.1623 - val_accuracy: 0.5938\n",
      "Epoch 10/10\n",
      "7/7 [==============================] - 5s 729ms/step - loss: 0.9033 - accuracy: 0.6712 - val_loss: 2.0572 - val_accuracy: 0.3490\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x29354f250>"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Fit the model\n",
    "\n",
    "model.fit(trainImageData,\n",
    "          validation_data=testImageData,\n",
    "          epochs=10,\n",
    "          steps_per_epoch = len(trainImageData.filenames) // trainImageData.batch_size,\n",
    "          validation_steps= len(testImageData.filenames) // testImageData.batch_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python 3.10(tensor_env)",
   "language": "python",
   "name": "tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
